{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "\n",
    "# 训练过程中应用权重衰减\n",
    "for inputs, labels in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # 计算权重衰减项的梯度并添加到参数的梯度上\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            param.grad.add(param.data, lambda=optimizer.param_groups[0]['weight_decay'])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        self.fc = nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc((input @ self.weight) + self.bias)\n",
    "    \n",
    "    \n",
    "m = MyLinear(4, 3)\n",
    "sample_input = torch.randn(4)\n",
    "m(sample_input)\n",
    "for name,m in m.named_parameters():\n",
    "    print(name)\n",
    "    \n",
    "\n",
    "    loss.backward()\n",
    "sample_input = torch.ones(4)*1\n",
    "y = net(sample_input)\n",
    "y.backward()\n",
    "\n",
    "# 对模型的梯度进行裁剪\n",
    "nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4c87e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        self.fc = nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc((input @ self.weight) + self.bias)\n",
    "    \n",
    "# using modules to building blocks\n",
    "net = nn.Sequential()\n",
    "net.add_module('1', MyLinear(4, 3))\n",
    "net.add_module('3', nn.Linear(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "46884289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5650, -1.0272, -0.4034],\n",
      "        [-1.5650, -1.0272, -0.4034],\n",
      "        [-1.5650, -1.0272, -0.4034],\n",
      "        [-1.5650, -1.0272, -0.4034]])\n",
      "tensor([-1.5650, -1.0272, -0.4034])\n",
      "tensor([[-0.1358, -0.1905, -0.4474],\n",
      "        [ 3.8544,  5.4059, 12.6960],\n",
      "        [-0.0940, -0.1319, -0.3097]])\n",
      "tensor([-0.1178,  3.3428, -0.0815])\n",
      "tensor([[-11.6470, -13.3479,   6.3779]])\n",
      "tensor([7.2874])\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.ones(4)*1\n",
    "y = net(sample_input)\n",
    "y.backward()\n",
    "for m in net.parameters():\n",
    "    print(m.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be623a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0617, -0.0405, -0.0159],\n",
      "        [-0.0617, -0.0405, -0.0159],\n",
      "        [-0.0617, -0.0405, -0.0159],\n",
      "        [-0.0617, -0.0405, -0.0159]])\n",
      "tensor([-0.0617, -0.0405, -0.0159])\n",
      "tensor([[-0.0054, -0.0075, -0.0176],\n",
      "        [ 0.1520,  0.2132,  0.5008],\n",
      "        [-0.0037, -0.0052, -0.0122]])\n",
      "tensor([-0.0046,  0.1319, -0.0032])\n",
      "tensor([[-0.4594, -0.5265,  0.2516]])\n",
      "tensor([0.2874])\n"
     ]
    }
   ],
   "source": [
    "norm = 0\n",
    "for m in net.parameters():\n",
    "    norm += torch.sum(m.grad**2)\n",
    "norm = torch.sqrt(norm)\n",
    "if norm>1:\n",
    "    for m in net.parameters():\n",
    "        m.grad[:] *= 1/norm\n",
    "\n",
    "for m in net.parameters():\n",
    "    print(m.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "254f74d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0617, -0.0405, -0.0159],\n",
      "        [-0.0617, -0.0405, -0.0159],\n",
      "        [-0.0617, -0.0405, -0.0159],\n",
      "        [-0.0617, -0.0405, -0.0159]])\n",
      "tensor([-0.0617, -0.0405, -0.0159])\n",
      "tensor([[-0.0054, -0.0075, -0.0176],\n",
      "        [ 0.1520,  0.2132,  0.5008],\n",
      "        [-0.0037, -0.0052, -0.0122]])\n",
      "tensor([-0.0046,  0.1319, -0.0032])\n",
      "tensor([[-0.4594, -0.5265,  0.2516]])\n",
      "tensor([0.2874])\n"
     ]
    }
   ],
   "source": [
    "nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "for m in net.parameters():\n",
    "    print(m.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c133f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e898b9c5",
   "metadata": {},
   "source": [
    "## [**梯度裁剪**]\n",
    "\n",
    "对于长度为$T$的序列，我们在迭代中计算这$T$个时间步上的梯度，\n",
    "将会在反向传播过程中产生长度为$\\mathcal{O}(T)$的矩阵乘法链。\n",
    "如 :numref:`sec_numerical_stability`所述，\n",
    "当$T$较大时，它可能导致数值不稳定，\n",
    "例如可能导致梯度爆炸或梯度消失。\n",
    "因此，循环神经网络模型往往需要额外的方式来支持稳定训练。\n",
    "\n",
    "一般来说，当解决优化问题时，我们对模型参数采用更新步骤。\n",
    "假定在向量形式的$\\mathbf{x}$中，\n",
    "或者在小批量数据的负梯度$\\mathbf{g}$方向上。\n",
    "例如，使用$\\eta > 0$作为学习率时，在一次迭代中，\n",
    "我们将$\\mathbf{x}$更新为$\\mathbf{x} - \\eta \\mathbf{g}$。\n",
    "如果我们进一步假设目标函数$f$表现良好，\n",
    "即函数$f$在常数$L$下是*利普希茨连续的*（Lipschitz continuous）。\n",
    "也就是说，对于任意$\\mathbf{x}$和$\\mathbf{y}$我们有：\n",
    "\n",
    "$$|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|.$$\n",
    "\n",
    "在这种情况下，我们可以安全地假设：\n",
    "如果我们通过$\\eta \\mathbf{g}$更新参数向量，则\n",
    "\n",
    "$$|f(\\mathbf{x}) - f(\\mathbf{x} - \\eta\\mathbf{g})| \\leq L \\eta\\|\\mathbf{g}\\|,$$\n",
    "\n",
    "这意味着我们不会观察到超过$L \\eta \\|\\mathbf{g}\\|$的变化。\n",
    "这既是坏事也是好事。\n",
    "坏的方面，它限制了取得进展的速度；\n",
    "好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。\n",
    "\n",
    "有时梯度可能很大，从而优化算法可能无法收敛。\n",
    "我们可以通过降低$\\eta$的学习率来解决这个问题。\n",
    "但是如果我们很少得到大的梯度呢？\n",
    "在这种情况下，这种做法似乎毫无道理。\n",
    "一个流行的替代方案是通过将梯度$\\mathbf{g}$投影回给定半径\n",
    "（例如$\\theta$）的球来裁剪梯度$\\mathbf{g}$。\n",
    "如下式：\n",
    "\n",
    "(**$$\\mathbf{g} \\leftarrow \\min\\left(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}.$$**)\n",
    "\n",
    "通过这样做，我们知道梯度范数永远不会超过$\\theta$，\n",
    "并且更新后的梯度完全与$\\mathbf{g}$的原始方向对齐。\n",
    "它还有一个值得拥有的副作用，\n",
    "即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响，\n",
    "这赋予了模型一定程度的稳定性。\n",
    "梯度裁剪提供了一个快速修复梯度爆炸的方法，\n",
    "虽然它并不能完全解决问题，但它是众多有效的技术之一。\n",
    "\n",
    "下面我们定义一个函数来裁剪模型的梯度，\n",
    "模型是从零开始实现的模型或由高级API构建的模型。\n",
    "我们在此计算了所有模型参数的梯度的范数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "477a6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d44246",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in net.parameters() if p.requires_grad]\n",
    "norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e11767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74a36d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0617, -0.0405, -0.0159],\n",
      "        [-0.0617, -0.0405, -0.0159],\n",
      "        [-0.0617, -0.0405, -0.0159],\n",
      "        [-0.0617, -0.0405, -0.0159]])\n",
      "tensor([-0.0617, -0.0405, -0.0159])\n",
      "tensor([[-0.0054, -0.0075, -0.0176],\n",
      "        [ 0.1520,  0.2132,  0.5008],\n",
      "        [-0.0037, -0.0052, -0.0122]])\n",
      "tensor([-0.0046,  0.1319, -0.0032])\n",
      "tensor([[-0.4594, -0.5265,  0.2516]])\n",
      "tensor([0.2874])\n"
     ]
    }
   ],
   "source": [
    "grad_clipping(net, theta=1)\n",
    "for m in net.parameters():\n",
    "    print(m.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch44",
   "language": "python",
   "name": "torch44"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

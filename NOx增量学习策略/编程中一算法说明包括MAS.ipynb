{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c03ad0",
   "metadata": {},
   "source": [
    "### 1 根据离散概率分布采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38056e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['雷神']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    " # input: probability distribution and correspondence\n",
    "list_probability = [0.005, 0.015, 0.08, 0.25, 0.3, 0.25, 0.08, 0.015, 0.005]\n",
    "list_player_role = ['黑寡妇', '蜘蛛侠', '绿巨人', '雷神', '钢铁侠', '奇异博士', '美国队长', '黑豹', '鹰眼']\n",
    "# sampling\n",
    "result = random.choices(list_player_role, weights=list_probability, k=1)\n",
    "# output: sampling one by probability distribution\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80b5c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_probability' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m p \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.005\u001b[39m, \u001b[38;5;241m0.015\u001b[39m, \u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m0.015\u001b[39m, \u001b[38;5;241m0.005\u001b[39m]\n\u001b[0;32m      4\u001b[0m index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(p))\n\u001b[1;32m----> 5\u001b[0m probability_index \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoices(index, weights\u001b[38;5;241m=\u001b[39m\u001b[43mlist_probability\u001b[49m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(probability_index)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_probability' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "p = [0.005, 0.015, 0.08, 0.25, 0.3, 0.25, 0.08, 0.015, 0.005]\n",
    "index = np.arange(len(p))\n",
    "probability_index = random.choices(index, weights=list_probability, k=4)\n",
    "print(probability_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcd3b65",
   "metadata": {},
   "source": [
    "#### 线性抽样概率设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57547d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 500\n",
    "batch_size = 128\n",
    "p = np.arange(num_sample,0,-1)\n",
    "p = p/sum(p)\n",
    "index = np.random.choice(np.arange(num_sample), replace=False, size=batch_size, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65796bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_error = [0.5]*100\n",
    "one_step_error.pop()\n",
    "one_step_error.insert(0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62910589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_step_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5e81a",
   "metadata": {},
   "source": [
    "### 网格搜索超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a224b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "    def __init__(self, param:{}):\n",
    "        self.param = param\n",
    "        self.param_num = len(self.param)\n",
    "        self.param_keys = list(self.param.keys())\n",
    "        self.param_comb = []\n",
    "        self.get_param()\n",
    "        \n",
    "    def get_param(self,param_dict={}, current_num=0):\n",
    "        if current_num==self.param_num:\n",
    "            self.param_comb.append(param_dict)\n",
    "        else:\n",
    "            for param in self.param[self.param_keys[current_num]]:\n",
    "                param_dict.update({self.param_keys[current_num]:param})\n",
    "                self.get_param(param_dict=param_dict.copy(), current_num=current_num+1)\n",
    "\n",
    "### 参数不同模型不同，遍历参数组合，重构模型\n",
    "### 保留每个epoch验证集和测试集的损失，\n",
    "### 记录每个模型在150个epoch下，验证集loss最小值\n",
    "param = {'hidden_size':[16, 32, 64],'layers':[2, 3, 4],'weight_decay':[1e-1,1e-2,1e-3,1e-4],'dropout':[0.1,0.2,0.4,0.6]}\n",
    "hyperparams = Hyperparams(param)\n",
    "# grid_search\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('training on', device)\n",
    "# loss = nn.L1Loss(reduction ='sum')\n",
    "# epochs = 150\n",
    "# val_loss_min = np.zeros((len(hyperparams.param_comb), 5))\n",
    "# train_loss_min = np.zeros((len(hyperparams.param_comb), 5))\n",
    "# for i,p in enumerate(hyperparams.param_comb):\n",
    "#     # 重建网络\n",
    "#     print(p)\n",
    "#     net = RNNModel(feature_size=16, hidden_size=p['hidden_size'], num_layers=p['layers'], dropout=p['dropout'])\n",
    "#     net.to(dtype=torch.float32, device=device)\n",
    "#     net.apply(init_net)\n",
    "#     optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=p['weight_decay'])\n",
    "#     # 每个参数下训练5次\n",
    "#     for j in range(5):\n",
    "#         val_loss_min[i,j], train_loss_min[i,j] = train_net(net, train_dl, val_dl, optimizer, loss, device, epochs)\n",
    "#     print(val_loss_min[i,:], train_loss_min[i,:])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28dfe712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# 超参数优化结果\n",
    "val_loss_min = np.load(r'result\\val_loss_min.npy')\n",
    "train_loss_min = np.load(r'result\\train_loss_min.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13f0aa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 64, 'layers': 2, 'weight_decay': 0.001, 'dropout': 0.1}\n",
      "val_mean 1.8475780181884764\n",
      "val_std 0.7284600360033135\n",
      "train_mean 1.2901618954755258\n",
      "train_std 0.1709701254689933\n",
      "{'hidden_size': 64, 'layers': 2, 'weight_decay': 0.001, 'dropout': 0.2}\n",
      "val_mean 2.0550709974500867\n",
      "val_std 0.44853302974009246\n",
      "train_mean 1.8151175628883252\n",
      "train_std 0.41698171972144915\n",
      "{'hidden_size': 32, 'layers': 2, 'weight_decay': 0.01, 'dropout': 0.1}\n",
      "val_mean 2.123810840182834\n",
      "val_std 0.43803324409582656\n",
      "train_mean 1.7163538764179616\n",
      "train_std 0.3523229663614422\n",
      "{'hidden_size': 32, 'layers': 2, 'weight_decay': 0.001, 'dropout': 0.1}\n",
      "val_mean 2.2155883009168837\n",
      "val_std 0.5816161944579709\n",
      "train_mean 1.8081633330082547\n",
      "train_std 0.37824978574575824\n",
      "{'hidden_size': 64, 'layers': 2, 'weight_decay': 0.001, 'dropout': 0.4}\n",
      "val_mean 2.263553675333659\n",
      "val_std 0.21309375907809852\n",
      "train_mean 2.041412507209225\n",
      "train_std 0.18176179739358814\n",
      "{'hidden_size': 32, 'layers': 3, 'weight_decay': 0.0001, 'dropout': 0.1}\n",
      "val_mean 2.2900274793836806\n",
      "val_std 0.18752432319384488\n",
      "train_mean 1.7329238841844643\n",
      "train_std 0.12777347089326127\n",
      "{'hidden_size': 64, 'layers': 2, 'weight_decay': 0.0001, 'dropout': 0.1}\n",
      "val_mean 2.2999612748887803\n",
      "val_std 0.9360206200638492\n",
      "train_mean 1.964688416633053\n",
      "train_std 0.5356967136968107\n"
     ]
    }
   ],
   "source": [
    "val_loss_min_mean = val_loss_min.mean(1)\n",
    "val_loss_min_std = val_loss_min.std(1)\n",
    "train_loss_min_mean = train_loss_min.mean(1)\n",
    "train_loss_min_std = train_loss_min.std(1)\n",
    "for i in val_loss_min_mean.argsort()[0:7]:\n",
    "    print(hyperparams.param_comb[i])\n",
    "    print('val_mean',val_loss_min_mean[i])\n",
    "    print('val_std',val_loss_min_std[i])\n",
    "    print('train_mean',train_loss_min_mean[i])\n",
    "    print('train_std',train_loss_min_std[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628645e7",
   "metadata": {},
   "source": [
    "### MAS源代码解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if continual_learning and loss_window_mean < self.loss_window_mean_threshold and loss_window_variance < self.loss_window_variance_threshold and new_peak_detected:\n",
    "if continual_learning and new_peak_detected:\n",
    "    count_updates+=1\n",
    "    update_tags.append(0.01)\n",
    "    last_loss_window_mean=loss_window_mean\n",
    "    last_loss_window_variance=loss_window_variance\n",
    "    new_peak_detected=False\n",
    "    # calculate importance weights and update star_variables\n",
    "    gradients=[0 for p in self.model.parameters()]\n",
    "    # calculate imporatance based on each sample in the hardbuffer\n",
    "    for sx in [_['state'] for _ in hard_buffer]:\n",
    "        self.model.zero_grad()\n",
    "        y_pred=self.model(torch.from_numpy(np.asarray(sx).reshape(-1,self.dim)).type(torch.float32))\n",
    "        torch.norm(y_pred, 2, dim=1).backward()\n",
    "        for pindex, p in enumerate(self.model.parameters()):\n",
    "            g=p.grad.data.clone().detach().numpy()\n",
    "            gradients[pindex]+=np.abs(g)\n",
    "            \n",
    "            \n",
    "# 训练过程\n",
    "for gs in range(self.gradient_steps):\n",
    "    # evaluate the new batch\n",
    "    y_pred = self.model(torch.from_numpy(np.asarray(x).reshape(-1,self.dim)).type(torch.float32))\n",
    "    y_sup=torch.zeros(len(y),2).scatter_(1,torch.from_numpy(np.asarray(y).reshape(-1,1)).type(torch.LongTensor),1.).type(torch.FloatTensor)\n",
    "    recent_loss = self.loss_fn(y_pred,y_sup)\n",
    "    total_loss = torch.sum(self.loss_fn(y_pred,y_sup))\n",
    "\n",
    "    if use_hard_buffer and len(hard_buffer) != 0:\n",
    "        # evaluate hard buffer\n",
    "        yh_pred = self.model(torch.from_numpy(np.asarray(xh).reshape(-1,self.dim)).type(torch.float32))\n",
    "        yh_sup=torch.zeros(len(yh),2).scatter_(1,torch.from_numpy(np.asarray(yh).reshape(-1,1)).type(torch.LongTensor),1.).type(torch.FloatTensor)\n",
    "\n",
    "        hard_loss = self.loss_fn(yh_pred,yh_sup)\n",
    "        total_loss += torch.sum(self.loss_fn(yh_pred,yh_sup))\n",
    "\n",
    "    # keep train loss for loss window\n",
    "    if gs==0: first_train_loss=total_loss.detach().numpy()\n",
    "\n",
    "    # add MAS regularization to the training objective\n",
    "    if continual_learning and len(star_variables)!=0 and len(omegas)!=0:\n",
    "        for pindex, p in enumerate(self.model.parameters()):\n",
    "            total_loss+=self.MAS_weight/2.*torch.sum(torch.from_numpy(omegas[pindex]).type(torch.float32)*(p-star_variables[pindex])**2)\n",
    "\n",
    "    # train self.model\n",
    "    self.optimizer.zero_grad()\n",
    "    torch.sum(total_loss).backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # save training accuracy on total batch\n",
    "    if use_hard_buffer and len(hard_buffer) != 0:\n",
    "    xt=x+xh\n",
    "    yt=y+yh\n",
    "    else:\n",
    "    xt=x[:]\n",
    "    yt=y[:]\n",
    "    yt_pred = self.model(torch.from_numpy(np.asarray(xt).reshape(-1,self.dim)).type(torch.float32))\n",
    "    accuracy = np.mean(np.argmax(yt_pred.detach().numpy(),axis=1)==yt)\n",
    "    msg+=' recent loss: {0:0.3f}'.format(np.mean(recent_loss.detach().numpy()))\n",
    "    if use_hard_buffer and len(hard_buffer) != 0:\n",
    "    msg+=' hard loss: {0:0.3f}'.format(np.mean(hard_loss.detach().numpy()))\n",
    "    losses.append(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a31d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算重要性权重\n",
    "gradients=[0 for p in net.parameters()]   # 初始化，用来保存单个样本在线更新时的梯度\n",
    "if continual_learning and error<thd:  # 仅在误差较小的情况下保持网络稳定性权值\n",
    "    # 计算各个参数的重要性权重\n",
    "    # 分析重要性权重 并做分布调整 如果有需要的话\n",
    "    # calculate imporatance based on each sample\n",
    "    # 统计更新次数\n",
    "    count_updates += 1 \n",
    "    # 梯度清零\n",
    "    net.zero_grad()\n",
    "    # 获得预测结果,因为是在线的方式这是单个样本\n",
    "    torch.norm(y_pred, 2, dim=1).backward() # 多步的稳定性\n",
    "    y_pred=net(X);  # 这是多步预测结果呀\n",
    "    # 梯度backward()\n",
    "    y_pred.backward()\n",
    "    # 保存grad，对梯度做分布map处理\n",
    "    for pindex, p in enumerate(net.parameters()):\n",
    "        g=p.grad.data.clone().detach().numpy()\n",
    "        gradients[pindex]+=np.abs(g)\n",
    "\n",
    "# update the running average of the importance weights        \n",
    "# 文章中采用的平均权重，这里采用时间衰减权重\n",
    "    omegas_old = omegas[:]  # 上次的omegas，内存地址没变\n",
    "    omegas=[]  # 当前的omegas\n",
    "    star_variables=[]\n",
    "    for pindex, p in enumerate(net.parameters()):\n",
    "        if len(omegas_old) != 0:   # 不是初始化omegas\n",
    "            omegas.append(1/count_updates*gradients[pindex]+(1-1/count_updates)*omegas_old[pindex])\n",
    "        else:\n",
    "            omegas.append(gradients[pindex])\n",
    "        star_variables.append(p.data.clone().detach())  # 保存此时的参数，用来构造Loss\n",
    "\n",
    "\n",
    "# 计算损失，训练模型，源代码中是最近buffer满了之后再训练\n",
    "# Train model on replaybuffer when it is full:\n",
    "for epoch in range(epochs):\n",
    "    # evaluate the new batch\n",
    "    y_pred = net(x)\n",
    "    recent_loss = loss(y_pred,y)\n",
    "    total_loss = torch.sum(loss(y_pred,y_sup))  # 这里求和\n",
    "    \n",
    "    # evaluate hard buffer\n",
    "    yh_pred = net(xh)\n",
    "    hard_loss = loss(yh_pred,yh)\n",
    "    total_loss += torch.sum(loss_fn(yh_pred,yh_sup))\n",
    "    \n",
    "    # add MAS regularization to the training objective\n",
    "    if continual_learning:\n",
    "        for pindex, p in enumerate(net.parameters()):\n",
    "            total_loss += l*torch.sum(omegas[pindex]*(p-star_variables[pindex])**2)\n",
    "# train self.model\n",
    "optimizer.zero_grad()\n",
    "torch.sum(total_loss).backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0c410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6b7d7fc",
   "metadata": {},
   "source": [
    "### 转换操作\n",
    ".detach().numpy()\\\n",
    "torch.from_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 绘图\n",
    "config = {\n",
    "\"font.family\":'serif',  \n",
    "\"font.size\": 14,       #14 放大是16\n",
    "\"mathtext.fontset\":'stix',\n",
    "\"xtick.direction\":\"in\",\n",
    "\"ytick.direction\":\"in\",\n",
    "#\"font.serif\": ['SimSun'],\n",
    "\"font.family\":\"times new roman\",\n",
    "}\n",
    "rcParams.update(config)\n",
    "color_list = plt.cm.tab10(np.linspace(0, 1, 12))\n",
    "x = np.arange(1,16)\n",
    "fig, ax1 = plt.subplots(2,2, figsize=(5,4), dpi=160)\n",
    "\n",
    "points = [16000,18000,23000,25000]\n",
    "point = points[0]\n",
    "ax1[0,0].plot(x, y_true[point-1,:], color=color_list[4], linestyle='-', linewidth=2)\n",
    "ax1[0,0].plot(x, y_pred[point-1,:], color=color_list[0], linestyle='-.', linewidth=2)\n",
    "ax1[0,0].set_title('No.10')\n",
    "\n",
    "\n",
    "point = points[1]\n",
    "ax1[0,1].plot(x, y_true[point-1,:], color=color_list[4], linestyle='-', linewidth=2)\n",
    "ax1[0,1].plot(x, y_pred[point-1,:], color=color_list[0], linestyle='-.', linewidth=2)\n",
    "ax1[0,1].set_title('No.460')\n",
    "\n",
    "point = points[2]\n",
    "ax1[1,0].plot(x, y_true[point-1,:], color=color_list[4], linestyle='-', linewidth=2)\n",
    "ax1[1,0].plot(x, y_pred[point-1,:], color=color_list[0], linestyle='-.', linewidth=2)\n",
    "\n",
    "ax1[1,0].set_title('No.990')\n",
    "\n",
    "point = points[3]   # 1380\n",
    "ax1[1,1].plot(x, y_true[point-1,:], color=color_list[4], linestyle='-', linewidth=2)\n",
    "ax1[1,1].plot(x, y_pred[point-1,:], color=color_list[0], linestyle='-.', linewidth=2)\n",
    "\n",
    "ax1[1,1].set_title('No.1580')\n",
    "#ax1[1,1].set_ylim([290,320])\n",
    "\n",
    "egend_font = {\"family\" : \"serif\"}\n",
    "fig.legend(['TRUE', 'LSTM', ],frameon=False,\\\n",
    "           fontsize='x-small', bbox_to_anchor=(1.18, 0.92))\n",
    "\n",
    "fig.text(0.5, 0, \"Horizon\", ha='center')\n",
    "fig.text(0, 0.5, \"NOx($\\mathrm{mg/m^{3}}$)\", va='center', rotation='vertical')   \n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch44",
   "language": "python",
   "name": "torch44"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

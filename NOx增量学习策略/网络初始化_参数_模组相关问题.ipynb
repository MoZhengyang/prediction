{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50897e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d16a063e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4423,  1.2361,  0.6878],\n",
       "        [-0.2776, -0.7045, -0.8003]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.nn.Parameter(torch.empty(2, 3))\n",
    "\n",
    "nn.init.uniform_(w , a=-1, b=1)\n",
    "nn.init.normal_(w, mean=0, std=1)\n",
    "nn.init.ones_(w)\n",
    "nn.init.xavier_uniform_(w, gain=1)\n",
    "nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8039739",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w.uniform_(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6ac70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(1, 3, kernel_size=1)\n",
    "nn.init.kaiming_normal_(conv.weight, mode='fan_in')\n",
    "nn.init.constant_(conv.bias, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "973b286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0\n",
      "weight_hh_l0\n",
      "bias_ih_l0\n",
      "bias_hh_l0\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.LSTM(input_size=12, hidden_size=128, num_layers=1, bidirectional=False)\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(name)\n",
    "    if name.startswith(\"weight\"):\n",
    "        nn.init.xavier_normal_(param)\n",
    "    else:\n",
    "        nn.init.zeros_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb9e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_parameters(self):\n",
    "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    if self.bias is not None:\n",
    "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "def reset_parameters(self):\n",
    "    n = self.in_channels\n",
    "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    if self.bias is not None:\n",
    "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "def reset_parameters(self):\n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for weight in self.parameters():\n",
    "        init.uniform_(weight, -stdv, stdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443e000",
   "metadata": {},
   "source": [
    "The full set of parameters registered by the module can be iterated through \\\n",
    "via a call to parameters() or named_parameters(),\\\n",
    "where the latter includes each parameter’s name:\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055b2c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        self.fc = nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc((input @ self.weight) + self.bias)\n",
    "    \n",
    "    \n",
    "m = MyLinear(4, 3)\n",
    "sample_input = torch.randn(4)\n",
    "m(sample_input)\n",
    "for name,m in m.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95631a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8476fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8255cce3",
   "metadata": {},
   "source": [
    "### Modules as Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a2f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using modules to building blocks\n",
    "net = nn.Sequential()\n",
    "net.add_module('1', MyLinear(4, 3))\n",
    "net.add_module('3', nn.Linear(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7b078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2592, -0.0934,  0.3342],\n",
      "        [ 0.2592, -0.0934,  0.3342],\n",
      "        [ 0.2592, -0.0934,  0.3342],\n",
      "        [ 0.2592, -0.0934,  0.3342]])\n",
      "tensor([ 0.2592, -0.0934,  0.3342])\n",
      "tensor([[ 1.1968,  1.1262, -0.4764],\n",
      "        [ 0.2241,  0.2109, -0.0892],\n",
      "        [-1.0052, -0.9459,  0.4002]])\n",
      "tensor([-0.4934, -0.0924,  0.4144])\n",
      "tensor([[1.1604, 0.6113, 0.8149]])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.ones(4)*1\n",
    "y = net(sample_input)\n",
    "y.backward()\n",
    "for m in net.parameters():\n",
    "    print(m.grad)\n",
    "\n",
    "    \n",
    "optimizer = torch.optim.Adam(net.parameters())  ## 清除梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68782af",
   "metadata": {},
   "source": [
    "### 初始化过程相当于p.grad置为None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e53ddebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': [Parameter containing:\n",
      "tensor([[ 0.4334, -2.0267,  0.2222],\n",
      "        [-2.9178,  1.7465, -0.3632],\n",
      "        [ 0.6030, -1.7102, -0.6228],\n",
      "        [-1.3348,  0.6843,  0.6407]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7906, -0.9763,  1.0886], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2988, -0.2928, -0.2925],\n",
      "        [-0.4478,  0.1074,  0.2529],\n",
      "        [ 0.1699, -0.5501,  0.5145]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0498, -0.4739, -0.5253], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.4934, -0.0924,  0.4144]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.5047], requires_grad=True)], 'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "tensor([[ 0.2592, -0.0934,  0.3342],\n",
      "        [ 0.2592, -0.0934,  0.3342],\n",
      "        [ 0.2592, -0.0934,  0.3342],\n",
      "        [ 0.2592, -0.0934,  0.3342]])\n",
      "tensor([ 0.2592, -0.0934,  0.3342])\n",
      "tensor([[ 1.1968,  1.1262, -0.4764],\n",
      "        [ 0.2241,  0.2109, -0.0892],\n",
      "        [-1.0052, -0.9459,  0.4002]])\n",
      "tensor([-0.4934, -0.0924,  0.4144])\n",
      "tensor([[1.1604, 0.6113, 0.8149]])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "for group in optimizer.param_groups:\n",
    "    print(group)\n",
    "    for p in group['params']:\n",
    "        print(p.grad)\n",
    "        if p.grad is not None:\n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c4c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      "None\n",
      "1.bias\n",
      "None\n",
      "1.fc.weight\n",
      "None\n",
      "1.fc.bias\n",
      "None\n",
      "3.weight\n",
      "None\n",
      "3.bias\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for name,parameter in net.named_parameters():\n",
    "    print(name)\n",
    "    print(parameter.grad)\n",
    "    with torch.no_grad():\n",
    "        if parameter.grad is not None:\n",
    "            parameter.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1bb7b2",
   "metadata": {},
   "source": [
    "### Simple submodule children() named_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "190a0ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l0\n",
      "l1\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(4, 3)\n",
    "        self.l1 = nn.Linear(3, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.l0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "net = Net()\n",
    "for name, child in net.named_children():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e646829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', BigNet(\n",
      "  (l1): MyLinear(\n",
      "    (fc): Linear(in_features=3, out_features=3, bias=True)\n",
      "  )\n",
      "  (net): Net(\n",
      "    (l0): Linear(in_features=4, out_features=3, bias=True)\n",
      "    (l1): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      "))\n",
      "('l1', MyLinear(\n",
      "  (fc): Linear(in_features=3, out_features=3, bias=True)\n",
      "))\n",
      "('l1.fc', Linear(in_features=3, out_features=3, bias=True))\n",
      "('net', Net(\n",
      "  (l0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (l1): Linear(in_features=3, out_features=1, bias=True)\n",
      "))\n",
      "('net.l0', Linear(in_features=4, out_features=3, bias=True))\n",
      "('net.l1', Linear(in_features=3, out_features=1, bias=True))\n"
     ]
    }
   ],
   "source": [
    "class BigNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = MyLinear(5, 4)\n",
    "        self.net = Net()\n",
    "    def forward(self, x):\n",
    "        return self.net(self.l1(x))\n",
    "\n",
    "big_net = BigNet()\n",
    "for module in big_net.named_modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c12e47",
   "metadata": {},
   "source": [
    "### 动态定网络submodules. The ModuleList and ModuleDict modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b28f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "        [nn.Linear(4, 4) for _ in range(num_layers)])\n",
    "        self.activations = nn.ModuleDict({\n",
    "        'relu': nn.ReLU(),\n",
    "        'lrelu': nn.LeakyReLU()\n",
    "        })\n",
    "        self.final = nn.Linear(4, 1)\n",
    "        \n",
    "    def forward(self, x, act):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "            x = self.activations[act](x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "dynamic_net = DynamicNet(3)\n",
    "sample_input = torch.randn(4)\n",
    "output = dynamic_net(sample_input, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7af80d",
   "metadata": {},
   "source": [
    "###  its parameters consist of its direct parameters as well as the parameters of all submodules.\n",
    "使用parameters 和 named_parameters() 将会递归的调用子参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33217157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('linears.0.weight', Parameter containing:\n",
      "tensor([[ 0.4857,  0.2158, -0.3233, -0.4697],\n",
      "        [ 0.3397,  0.4831,  0.0161,  0.4129],\n",
      "        [ 0.4010,  0.2304,  0.3120,  0.2003],\n",
      "        [-0.3562,  0.0602, -0.0277,  0.0270]], requires_grad=True))\n",
      "('linears.0.bias', Parameter containing:\n",
      "tensor([ 0.4645,  0.0422, -0.2372, -0.2482], requires_grad=True))\n",
      "('linears.1.weight', Parameter containing:\n",
      "tensor([[ 0.3702,  0.1347,  0.2715, -0.0403],\n",
      "        [ 0.1754, -0.0897,  0.4280,  0.4035],\n",
      "        [-0.1338, -0.3020, -0.1539,  0.3424],\n",
      "        [ 0.2116,  0.1039,  0.3017,  0.0621]], requires_grad=True))\n",
      "('linears.1.bias', Parameter containing:\n",
      "tensor([ 0.1498,  0.0089, -0.0198,  0.2049], requires_grad=True))\n",
      "('linears.2.weight', Parameter containing:\n",
      "tensor([[-2.2388e-01,  1.9924e-01, -3.7828e-01,  3.1292e-04],\n",
      "        [-2.2334e-02, -8.1629e-02,  4.8860e-01, -3.2840e-01],\n",
      "        [ 1.5269e-01,  1.7929e-01, -2.4909e-01, -5.4804e-03],\n",
      "        [ 7.5740e-02, -3.4095e-02, -1.7930e-01, -4.2932e-01]],\n",
      "       requires_grad=True))\n",
      "('linears.2.bias', Parameter containing:\n",
      "tensor([ 0.3134,  0.1849,  0.2814, -0.1774], requires_grad=True))\n",
      "('final.weight', Parameter containing:\n",
      "tensor([[ 0.3414,  0.4703, -0.1631, -0.0135]], requires_grad=True))\n",
      "('final.bias', Parameter containing:\n",
      "tensor([-0.1243], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# parameter_registers = []\n",
    "for parameters in dynamic_net.named_parameters():\n",
    "    print(parameters)\n",
    "#     with torch.no_grad():\n",
    "#         parameter_registers.append(parameters[1].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0497d62f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameter_registers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mparameter_registers\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameter_registers' is not defined"
     ]
    }
   ],
   "source": [
    "parameter_registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f02903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6f5ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicNet(\n",
       "  (linears): ModuleList(\n",
       "    (0-2): 3 x Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (activations): ModuleDict(\n",
       "    (relu): ReLU()\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (final): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(dynamic_net.parameters()).device\n",
    "dynamic_net.to(device=device, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1379bc35",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      2\u001b[0m dynamic_net\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdynamic_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m result\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch44\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mDynamicNet.forward\u001b[1;34m(self, x, act)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, act):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m linear \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears:\n\u001b[1;32m---> 14\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[act](x)\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(x)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch44\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch44\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(4)\n",
    "dynamic_net.to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "result = dynamic_net(torch.randn(4, device='cuda', dtype=torch.float32), act='relu')\n",
    "result.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03db78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e02ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linears.0.weight None\n",
      "linears.0.bias None\n",
      "linears.1.weight None\n",
      "linears.1.bias None\n",
      "linears.2.weight None\n",
      "linears.2.bias None\n",
      "final.weight None\n",
      "final.bias None\n"
     ]
    }
   ],
   "source": [
    "#grad_register = []\n",
    "for param in dynamic_net.named_parameters():\n",
    "    print(param[0],param[1].grad)\n",
    "    #with torch.no_grad():\n",
    "     #   grad_register.append(param[1].grad.detach().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2273f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in dynamic_net.named_parameters():\n",
    "    with torch.no_grad():\n",
    "        if param[1].grad is not None:\n",
    "            param[1].grad = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "989494c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_parameters(self):\n",
    "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    if self.bias is not None:\n",
    "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "def reset_parameters(self):\n",
    "    n = self.in_channels\n",
    "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    if self.bias is not None:\n",
    "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "def reset_parameters(self):\n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for weight in self.parameters():\n",
    "        init.uniform_(weight, -stdv, stdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe946b3",
   "metadata": {},
   "source": [
    "### 网络初始化函数及apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4dbae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "        [nn.Linear(4, 4) for _ in range(num_layers)])\n",
    "        self.activations = nn.ModuleDict({\n",
    "        'relu': nn.ReLU(),\n",
    "        'lrelu': nn.LeakyReLU()\n",
    "        })\n",
    "        self.cov = nn.Conv2d(2,2,kernel_size=1)\n",
    "        self.rnn = nn.LSTM(input_size=2, hidden_size=2, num_layers=2, dropout=0.3)\n",
    "        self.final = nn.Linear(4, 1)\n",
    "        \n",
    "    def forward(self, x, act):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "            x = self.activations[act](x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "dynamic_net = DynamicNet(3)\n",
    "\n",
    "### 初始化函数\n",
    "# Define a function to initialize Linear weights.\n",
    "# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n",
    "# @torch.no_grad()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "        if m.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            torch.nn.init.uniform_(m.bias, -bound, bound)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        n = m.in_channels\n",
    "        print(n)\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "        if m.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            torch.nn.init.uniform_(m.bias, -bound, bound)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        stdv = 1.0 / math.sqrt(m.hidden_size)\n",
    "        for weight in m.parameters():\n",
    "            print('w')\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2af1a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "('linears.0.weight', Parameter containing:\n",
      "tensor([[ 0.4184, -0.1598, -0.4926,  0.4890],\n",
      "        [ 0.2707,  0.3608, -0.0912, -0.1030],\n",
      "        [ 0.0489,  0.1224,  0.0129, -0.2595],\n",
      "        [ 0.4466,  0.2335, -0.0452, -0.4259]], requires_grad=True))\n",
      "('linears.0.bias', Parameter containing:\n",
      "tensor([-0.2463, -0.1744, -0.0834,  0.1384], requires_grad=True))\n",
      "('linears.1.weight', Parameter containing:\n",
      "tensor([[-0.0836,  0.1730, -0.3248, -0.1094],\n",
      "        [ 0.2260,  0.4911, -0.4224,  0.0520],\n",
      "        [ 0.2641, -0.0539,  0.1165,  0.3122],\n",
      "        [-0.4968,  0.2961, -0.4470,  0.0929]], requires_grad=True))\n",
      "('linears.1.bias', Parameter containing:\n",
      "tensor([ 0.3026, -0.2273,  0.0045, -0.4800], requires_grad=True))\n",
      "('linears.2.weight', Parameter containing:\n",
      "tensor([[-0.0308,  0.2818,  0.2492,  0.1336],\n",
      "        [-0.0410, -0.2049,  0.4064,  0.1477],\n",
      "        [ 0.1779,  0.2435, -0.1413, -0.0381],\n",
      "        [ 0.4718,  0.4475, -0.4963, -0.4267]], requires_grad=True))\n",
      "('linears.2.bias', Parameter containing:\n",
      "tensor([-0.1437,  0.1264, -0.2616,  0.2477], requires_grad=True))\n",
      "('cov.weight', Parameter containing:\n",
      "tensor([[[[-0.6344]],\n",
      "\n",
      "         [[-0.0855]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4558]],\n",
      "\n",
      "         [[-0.1805]]]], requires_grad=True))\n",
      "('cov.bias', Parameter containing:\n",
      "tensor([-0.1810,  0.1534], requires_grad=True))\n",
      "('rnn.weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.5417,  0.1586],\n",
      "        [ 0.0175, -0.0175],\n",
      "        [-0.5992, -0.2800],\n",
      "        [ 0.7043, -0.4733],\n",
      "        [ 0.3337, -0.2455],\n",
      "        [ 0.2393,  0.3139],\n",
      "        [ 0.5001, -0.4388],\n",
      "        [-0.0897, -0.4820]], requires_grad=True))\n",
      "('rnn.weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.1441, -0.3347],\n",
      "        [ 0.0699, -0.5817],\n",
      "        [-0.1596, -0.5907],\n",
      "        [-0.5983,  0.2442],\n",
      "        [ 0.0289, -0.2601],\n",
      "        [-0.3115,  0.6153],\n",
      "        [ 0.0751,  0.1965],\n",
      "        [ 0.7052,  0.6834]], requires_grad=True))\n",
      "('rnn.bias_ih_l0', Parameter containing:\n",
      "tensor([-0.1502,  0.3574, -0.0072, -0.5441,  0.4580, -0.0891, -0.0710,  0.1682],\n",
      "       requires_grad=True))\n",
      "('rnn.bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.5623, -0.1020, -0.0820,  0.3015,  0.1256,  0.3358,  0.6932, -0.2105],\n",
      "       requires_grad=True))\n",
      "('rnn.weight_ih_l1', Parameter containing:\n",
      "tensor([[ 0.5186, -0.6053],\n",
      "        [ 0.1051,  0.2281],\n",
      "        [-0.6540, -0.6972],\n",
      "        [-0.1974, -0.0909],\n",
      "        [ 0.4531, -0.2925],\n",
      "        [ 0.5819, -0.2447],\n",
      "        [ 0.1846,  0.5026],\n",
      "        [ 0.3761, -0.2860]], requires_grad=True))\n",
      "('rnn.weight_hh_l1', Parameter containing:\n",
      "tensor([[ 0.3227,  0.2036],\n",
      "        [-0.5758,  0.5021],\n",
      "        [ 0.6601, -0.0047],\n",
      "        [ 0.6887, -0.0687],\n",
      "        [-0.0235, -0.0735],\n",
      "        [ 0.1579,  0.3194],\n",
      "        [-0.5556, -0.2390],\n",
      "        [-0.5137, -0.1365]], requires_grad=True))\n",
      "('rnn.bias_ih_l1', Parameter containing:\n",
      "tensor([ 0.1989, -0.4460,  0.1211, -0.6772,  0.2809,  0.4277, -0.2327,  0.3989],\n",
      "       requires_grad=True))\n",
      "('rnn.bias_hh_l1', Parameter containing:\n",
      "tensor([-0.1047, -0.4818, -0.6552, -0.6357,  0.2131,  0.1120, -0.5294, -0.0927],\n",
      "       requires_grad=True))\n",
      "('final.weight', Parameter containing:\n",
      "tensor([[0.0319, 0.0168, 0.2700, 0.4806]], requires_grad=True))\n",
      "('final.bias', Parameter containing:\n",
      "tensor([0.3112], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "dynamic_net.apply(init_weights)\n",
    "for p in dynamic_net.named_parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c48371b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.5181, -0.0161],\n",
      "        [ 0.6578,  0.0111],\n",
      "        [-0.1733, -0.2237],\n",
      "        [ 0.3685,  0.6207],\n",
      "        [ 0.4737, -0.2573],\n",
      "        [-0.2658, -0.5630],\n",
      "        [-0.6517,  0.2302],\n",
      "        [ 0.2767,  0.2644]], requires_grad=True))\n",
      "('weight_hh_l0', Parameter containing:\n",
      "tensor([[-0.3235, -0.4423],\n",
      "        [ 0.1226,  0.1008],\n",
      "        [-0.5240, -0.6215],\n",
      "        [-0.0507,  0.0563],\n",
      "        [ 0.6154, -0.0559],\n",
      "        [ 0.2897, -0.0234],\n",
      "        [ 0.2582, -0.6910],\n",
      "        [-0.2747,  0.0940]], requires_grad=True))\n",
      "('bias_ih_l0', Parameter containing:\n",
      "tensor([ 0.3661,  0.6481,  0.5172,  0.4149,  0.2209,  0.5209, -0.4222, -0.0090],\n",
      "       requires_grad=True))\n",
      "('bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.5096,  0.6593,  0.6949,  0.1772,  0.6886,  0.3372,  0.4052, -0.4551],\n",
      "       requires_grad=True))\n",
      "('weight_ih_l1', Parameter containing:\n",
      "tensor([[ 0.4503,  0.4315],\n",
      "        [ 0.1963,  0.4462],\n",
      "        [-0.4434, -0.0621],\n",
      "        [ 0.2876, -0.5238],\n",
      "        [ 0.2541, -0.0027],\n",
      "        [ 0.1469, -0.5297],\n",
      "        [-0.3110,  0.2451],\n",
      "        [-0.0520,  0.0692]], requires_grad=True))\n",
      "('weight_hh_l1', Parameter containing:\n",
      "tensor([[ 0.6824,  0.1523],\n",
      "        [-0.2688, -0.0189],\n",
      "        [ 0.1773,  0.2393],\n",
      "        [ 0.1342,  0.1768],\n",
      "        [ 0.4956,  0.5826],\n",
      "        [-0.4954,  0.0615],\n",
      "        [-0.6804, -0.2682],\n",
      "        [ 0.3726, -0.2415]], requires_grad=True))\n",
      "('bias_ih_l1', Parameter containing:\n",
      "tensor([-0.1485,  0.3859, -0.4661,  0.5349, -0.5723,  0.1284, -0.6797,  0.1758],\n",
      "       requires_grad=True))\n",
      "('bias_hh_l1', Parameter containing:\n",
      "tensor([ 0.1798, -0.1596,  0.6342,  0.3977,  0.1650,  0.2368, -0.6606,  0.0680],\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.LSTM(input_size=2, hidden_size=2, num_layers=2, dropout=0.3)\n",
    "for m in rnn.named_parameters():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff97df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.1)\n",
    "for i in range(10000):\n",
    "    torch.manual_seed(i)\n",
    "    input = torch.randn(4)\n",
    "    output = net(input)\n",
    "    loss = torch.abs(output)\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d01a2737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1464, -0.1097, -0.2943, -0.3205],\n",
      "        [ 0.4544,  0.2091, -0.3286,  0.1542],\n",
      "        [ 0.2800,  0.1145, -0.3014, -0.0013]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2541,  0.3595,  0.3480], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2148, -0.2534,  0.3346]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-3.2252e-05], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# optimizer.zero_grad()  # net.zero_grad()\n",
    "for name,param in net.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2798c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "print(net.training)\n",
    "net.eval()\n",
    "print(net.training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a2008",
   "metadata": {},
   "source": [
    "### 保持模型 model state\n",
    "state_dict 保存的参数包括：\\\n",
    "Parameters: learnable aspects of computation; contained within the state_dict\\\n",
    "Persistent buffers: contained within the state_dict (i.e. serialized when saving & loading)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "86d1da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the module\n",
    "torch.save(net.state_dict(), 'net.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3cd47da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the module later on\n",
    "new_net = Net()\n",
    "new_net.load_state_dict(torch.load('net.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ae01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c5848f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean_register', torch.zeros(num_features))\n",
    "        self.mean = torch.zeros(num_features)\n",
    "    def forward(self, x):\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        self.mean_register = self.momentum * self.mean_register + (1.0 - self.momentum) * x\n",
    "        return self.mean\n",
    "    \n",
    "m = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "22ede43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('mean_register', tensor([-0.1269, -0.0470,  0.0229, -0.5168]))])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00539fb8",
   "metadata": {},
   "source": [
    "### to 方法将移动 参数 与 Buffers  to the specified device / dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "81908614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunningMean()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moves all module parameters and buffers to the specified device / dtype\n",
    "m.to(device='cuda', dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2e867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch44",
   "language": "python",
   "name": "torch44"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
